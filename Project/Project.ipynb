{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbcMhw-hvHwY",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression\n",
        "In this task, first of all, we have to create a function that generates 100 random points on the plane. Second we have to run both pocket and linear regression algorithm to get weights. Third, we need to test the performance of both algorithms. \n",
        "To do that, we have to use minimization square root error, which is\n",
        "\n",
        "$\\underset{w}{min}\\sum\\limits_{n=1}^N[y_{n}-(w^{T}x_{n})]^2$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqWXg0krc932",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Python libraries we need to use\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o43tRFo6iUIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dataset\n",
        "def dataset(size):\n",
        "  d = 2\n",
        "  w = np.random.randint(1,10,d)\n",
        "  b = np.random.randint(1,10) \n",
        "  x = np.random.uniform(-1,1,(size,d))*10\n",
        "  y = np.zeros(20)\n",
        "  #making a hypothesis line\n",
        "  h = x.dot(w) + b\n",
        "  labels = (h > 0)*1\n",
        "  flipping = np.random.randint(size, size=10)\n",
        "  #flipping indexes of randomly chosen 10 points N/10\n",
        "  for f in flipping:\n",
        "    if (labels[f] == 1):\n",
        "      labels[f] = 0\n",
        "    else:\n",
        "      labels[f] = 1\n",
        "  y = labels\n",
        "  return x, y, w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQvqA3h6yiQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def finding_err(x, w, y):\n",
        "    err = 0.0\n",
        "    for i in range(len(x)):\n",
        "        ypred = np.dot(x[i],w)\n",
        "        if(ypred > 0 and y[i] == -1): \n",
        "            err += 1\n",
        "        elif(ypred <= 0 and y[i] == 1): \n",
        "            err += 1\n",
        "    mean = err/x.size \n",
        "    return mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JMmXLQE0fwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pocket(x, y, w, iters): \n",
        "    \n",
        "    #updates\n",
        "    num_updates = [] \n",
        "    updates = 0\n",
        "    E = []\n",
        "    E_val=0.0\n",
        "    \n",
        "    #iterations\n",
        "    for it in range (iters):\n",
        "        \n",
        "        y_wrong = [] #saves indeces of the wrong predicted y\n",
        "        predict=[]\n",
        "        \n",
        "        for i in range(len(x)):\n",
        "            \n",
        "            if np.dot(x[i],w)>0:\n",
        "                predict.append(1)\n",
        "            else: predict.append(0)\n",
        "            \n",
        "            if predict[i] != y[i]:\n",
        "                y_wrong.append(i)\n",
        "                \n",
        "        #randomly choose the index of wrong classified x\n",
        "        pick = np.random.randint(len(y_wrong)) \n",
        "        \n",
        "        #new weight\n",
        "        weight = w + (y[pick]-predict[pick]) * x[pick]\n",
        "\n",
        "        E_old = finding_err(x, w, y)\n",
        "        E_new = finding_err(x, weight, y)\n",
        "\n",
        "        if (E_new < E_old): #compare old and new Error\n",
        "\n",
        "            E.append(E_new) \n",
        "            num_updates.append(updates)\n",
        "            updates += 1\n",
        "            w = np.copy(weight) #update weight to new weight\n",
        "            E_val=E_new\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWOvy3ny4PdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_reg(x, y):\n",
        "  y_new = y.reshape(len(y), 1)\n",
        "  inverse = np.linalg.inv(np.dot(x.T, x))\n",
        "  inv_x = np.dot(inverse, x.T)\n",
        "  w_lin = np.dot(inv_x, y_new)\n",
        "  return w_lin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT6MDcA-Wxjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generating dataset with random 100 points\n",
        "x_train, y_train, x_test, y_test = dataset(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxbqCorsXQrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Running pocket algorithm\n",
        "w = np.zeros(2)\n",
        "T = 1000\n",
        "w_poc = pocket(x_train, y_train, w, T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os9nYI3zobqu",
        "colab_type": "code",
        "outputId": "d2d9e93d-8ba0-495f-81cc-3e89f621e666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "w_lin=linear_reg(x_train, y_train)\n",
        "print ('Weights: ',w_lin)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:  [[0.02486019]\n",
            " [0.04194516]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjyKr2b8oqPW",
        "colab_type": "code",
        "outputId": "93efbe45-aeff-4e91-e32d-1694183e983d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Generating test dataset with 1000 points\n",
        "x_test, y_test, w_test, b_test=dataset(1000)\n",
        "\n",
        "E_pocket=finding_err(x_test, w_poc, y_test)\n",
        "E_pocket"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_tVoAGWpaya",
        "colab_type": "code",
        "outputId": "9807d6f5-d3b3-4e40-e031-9c48f6cb73d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "E_lin=finding_err(x_test, w_lin, y_test)\n",
        "E_lin"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.041"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d22R4n74psq7",
        "colab_type": "code",
        "outputId": "7bf2a716-5e94-4c19-b2e3-4b41d42c94df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#New experiment\n",
        "x_new, y_new, w_new, b_new = dataset(1000)\n",
        "\n",
        "E_poc_new = np.zeros(100)\n",
        "E_lin_new = np.zeros(100)\n",
        "w_new = np.zeros(3)\n",
        "n = np.zeros(100)\n",
        "\n",
        "for i in range (100):\n",
        "    \n",
        "    E_poc_new[i]=finding_err(x_new, w_poc, y_new)\n",
        "    E_lin_new[i]=finding_err(x_new, w_lin, y_test)\n",
        "    \n",
        "    n[i]=i\n",
        "    \n",
        "plt.scatter(n,E_poc_new*100, label='Pocket')\n",
        "plt.scatter(n,E_lin_new*100, label=\"Linear Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x8OWG1lztezDAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQJCLDvtr6uC",
        "colab_type": "text"
      },
      "source": [
        "From the above experiment we can clearly see that Pocket algorithm acts much better Linear Regression.\n",
        "However, linear regression works faster than pocket. Anyway both pocket and linear regression give small errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgYCUcqvr1L",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OaVXDrrv1Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "iris = load_iris()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzdK6gwyw7Aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(data):\n",
        "    x = iris.data\n",
        "    y = iris.target\n",
        "    \n",
        "    # random sampling\n",
        "    size = int(0.8*x.shape[0]) \n",
        "    train = random.sample(range(0,150),120)\n",
        "    x_train = x[train]\n",
        "    y_train = y[train]\n",
        "    \n",
        "    #get test by substracting train\n",
        "    test = list(set(range(0,150))-set(train)) \n",
        "    x_test = x[test]\n",
        "    y_test = y[test]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYCe7jWvy_m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_test, y_test = train_test_split(iris)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mZUxQlhzGz2",
        "colab_type": "code",
        "outputId": "6964da33-d01a-4401-d45d-857a857ab1a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "train_samp, d = x_train.shape\n",
        "test_samp = x_test.shape[0]\n",
        "\n",
        "train = random.sample(range(0,120),120)\n",
        "test = random.sample(range(0,30),30)\n",
        "\n",
        "#iters = 2000\n",
        "#iters = 0.001\n",
        "iters = 50\n",
        "lr = 0.0001\n",
        "\n",
        "weights = np.zeros(d+1)\n",
        "\n",
        "E_in = []\n",
        "E_test = []\n",
        "\n",
        "for it in range(iters):\n",
        "    E_ins = 0\n",
        "    for i in train:\n",
        "        ex_train = np.append(x_train[i],1)\n",
        "        cross_entropy_err = np.log(1+np.exp(-y_train[i]*2*np.dot(weights.T, ex_train)))\n",
        "        weights = weights + lr * cross_entropy_err\n",
        "        \n",
        "        E_ins += cross_entropy_err\n",
        "    E_in_ave = E_ins / train_samp\n",
        "    E_in.append(E_in_ave)\n",
        "    \n",
        "    E_tests = 0\n",
        "    for i in test:\n",
        "        ex_test = np.append(x_test[i],1)\n",
        "        cross_entropy_err = np.log(1+np.exp(-y_test[i]*2*np.dot(weights.T, ex_test)))\n",
        "        \n",
        "        E_tests += cross_entropy_err\n",
        "    E_test_ave = E_tests / test_samp\n",
        "    E_test.append(E_test_ave)\n",
        "    \n",
        "    print(\"EPOCH %d:\\tTrain error: %f\\t\\tTest error: %f\" % (it+1, E_in_ave, E_test_ave))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1:\tTrain error: 0.634817\t\tTest error: 0.582305\n",
            "EPOCH 2:\tTrain error: 0.537485\t\tTest error: 0.512243\n",
            "EPOCH 3:\tTrain error: 0.471527\t\tTest error: 0.466054\n",
            "EPOCH 4:\tTrain error: 0.424705\t\tTest error: 0.434350\n",
            "EPOCH 5:\tTrain error: 0.390075\t\tTest error: 0.411796\n",
            "EPOCH 6:\tTrain error: 0.363558\t\tTest error: 0.395247\n",
            "EPOCH 7:\tTrain error: 0.342659\t\tTest error: 0.382777\n",
            "EPOCH 8:\tTrain error: 0.325793\t\tTest error: 0.373162\n",
            "EPOCH 9:\tTrain error: 0.311910\t\tTest error: 0.365602\n",
            "EPOCH 10:\tTrain error: 0.300297\t\tTest error: 0.359553\n",
            "EPOCH 11:\tTrain error: 0.290448\t\tTest error: 0.354641\n",
            "EPOCH 12:\tTrain error: 0.281999\t\tTest error: 0.350599\n",
            "EPOCH 13:\tTrain error: 0.274682\t\tTest error: 0.347234\n",
            "EPOCH 14:\tTrain error: 0.268293\t\tTest error: 0.344404\n",
            "EPOCH 15:\tTrain error: 0.262675\t\tTest error: 0.342002\n",
            "EPOCH 16:\tTrain error: 0.257705\t\tTest error: 0.339945\n",
            "EPOCH 17:\tTrain error: 0.253286\t\tTest error: 0.338173\n",
            "EPOCH 18:\tTrain error: 0.249338\t\tTest error: 0.336635\n",
            "EPOCH 19:\tTrain error: 0.245798\t\tTest error: 0.335294\n",
            "EPOCH 20:\tTrain error: 0.242612\t\tTest error: 0.334116\n",
            "EPOCH 21:\tTrain error: 0.239736\t\tTest error: 0.333079\n",
            "EPOCH 22:\tTrain error: 0.237133\t\tTest error: 0.332161\n",
            "EPOCH 23:\tTrain error: 0.234772\t\tTest error: 0.331345\n",
            "EPOCH 24:\tTrain error: 0.232625\t\tTest error: 0.330618\n",
            "EPOCH 25:\tTrain error: 0.230669\t\tTest error: 0.329967\n",
            "EPOCH 26:\tTrain error: 0.228884\t\tTest error: 0.329384\n",
            "EPOCH 27:\tTrain error: 0.227253\t\tTest error: 0.328860\n",
            "EPOCH 28:\tTrain error: 0.225761\t\tTest error: 0.328388\n",
            "EPOCH 29:\tTrain error: 0.224393\t\tTest error: 0.327961\n",
            "EPOCH 30:\tTrain error: 0.223138\t\tTest error: 0.327575\n",
            "EPOCH 31:\tTrain error: 0.221986\t\tTest error: 0.327226\n",
            "EPOCH 32:\tTrain error: 0.220927\t\tTest error: 0.326909\n",
            "EPOCH 33:\tTrain error: 0.219953\t\tTest error: 0.326620\n",
            "EPOCH 34:\tTrain error: 0.219056\t\tTest error: 0.326358\n",
            "EPOCH 35:\tTrain error: 0.218229\t\tTest error: 0.326119\n",
            "EPOCH 36:\tTrain error: 0.217467\t\tTest error: 0.325901\n",
            "EPOCH 37:\tTrain error: 0.216764\t\tTest error: 0.325702\n",
            "EPOCH 38:\tTrain error: 0.216116\t\tTest error: 0.325520\n",
            "EPOCH 39:\tTrain error: 0.215517\t\tTest error: 0.325354\n",
            "EPOCH 40:\tTrain error: 0.214963\t\tTest error: 0.325202\n",
            "EPOCH 41:\tTrain error: 0.214451\t\tTest error: 0.325062\n",
            "EPOCH 42:\tTrain error: 0.213978\t\tTest error: 0.324934\n",
            "EPOCH 43:\tTrain error: 0.213541\t\tTest error: 0.324817\n",
            "EPOCH 44:\tTrain error: 0.213136\t\tTest error: 0.324710\n",
            "EPOCH 45:\tTrain error: 0.212761\t\tTest error: 0.324611\n",
            "EPOCH 46:\tTrain error: 0.212414\t\tTest error: 0.324521\n",
            "EPOCH 47:\tTrain error: 0.212093\t\tTest error: 0.324438\n",
            "EPOCH 48:\tTrain error: 0.211795\t\tTest error: 0.324361\n",
            "EPOCH 49:\tTrain error: 0.211519\t\tTest error: 0.324291\n",
            "EPOCH 50:\tTrain error: 0.211263\t\tTest error: 0.324226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEYs5YqpBIuA",
        "colab_type": "code",
        "outputId": "cd612fe7-5f7e-43a1-e331-f9c3ddcad739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.plot(range(iters), E_in, label='Training')\n",
        "plt.plot(range(iters), E_test, label='Testing')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nriegegAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1h2d0UeBpx8",
        "colab_type": "text"
      },
      "source": [
        "At T = 2000 and learning rate = 0.001 give awful results in our graph.\n",
        "In order to mitigate these values, learning rate = 0.0001 and T = 50, So that number of iterations and learning rate must decrease. Decreasing learning rate makes Logistic regression work slower, however, accuracy is much better. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWY_axAzCfIW",
        "colab_type": "text"
      },
      "source": [
        "# Practical design of a learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1ZMqDeFClWV",
        "colab_type": "code",
        "outputId": "dd94f0fa-9fc9-417c-9b8e-3a77ff622cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "print(digits.DESCR)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _digits_dataset:\n",
            "\n",
            "Optical recognition of handwritten digits dataset\n",
            "--------------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 5620\n",
            "    :Number of Attributes: 64\n",
            "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
            "    :Missing Attribute Values: None\n",
            "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
            "    :Date: July; 1998\n",
            "\n",
            "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
            "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
            "\n",
            "The data set contains images of hand-written digits: 10 classes where\n",
            "each class refers to a digit.\n",
            "\n",
            "Preprocessing programs made available by NIST were used to extract\n",
            "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
            "total of 43 people, 30 contributed to the training set and different 13\n",
            "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
            "4x4 and the number of on pixels are counted in each block. This generates\n",
            "an input matrix of 8x8 where each element is an integer in the range\n",
            "0..16. This reduces dimensionality and gives invariance to small\n",
            "distortions.\n",
            "\n",
            "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
            "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
            "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
            "1994.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
            "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
            "    Graduate Studies in Science and Engineering, Bogazici University.\n",
            "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
            "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
            "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
            "    Electrical and Electronic Engineering Nanyang Technological University.\n",
            "    2005.\n",
            "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
            "    Algorithm. NIPS. 2000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFU7TSSXDag4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = digits.data\n",
        "y = digits.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFR3PaS7E5dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def finding_linerr(x, w, y):\n",
        "    err = np.dot(y.T,y)-2*np.dot(w.T, np.dot(x.T,y))+np.dot(np.dot(w.T, x.T), np.dot(x,w))\n",
        "    return err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJtWq1EFGKgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lin_reg(x_train, x_test, y_train, y_test):\n",
        "    w = np.linalg.pinv(x_train.T.dot(x_train)).dot(x_train.T).dot(y_train)   \n",
        "    return finding_linerr(x_test, w, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2X3WX7bHBeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_reg(x_train, x_test, y_train, y_test):\n",
        "    train_samp, d = x_train.shape\n",
        "    test_samp = x_test.shape[0]\n",
        "\n",
        "    train = random.sample(range(0,120),120)\n",
        "    test = random.sample(range(0,30),30)\n",
        "\n",
        "    #iters = 2000\n",
        "    #iters = 0.001\n",
        "    iters = 50\n",
        "    lr = 0.0001\n",
        "\n",
        "    weights = np.zeros(d+1)\n",
        "\n",
        "    E_in = []\n",
        "    E_test = []\n",
        "    \n",
        "    for it in range(iters):\n",
        "        E_ins = 0\n",
        "        for i in train:\n",
        "          ex_train = np.append(x_train[i],1)\n",
        "          cross_entropy_err = np.log(1+np.exp(-y_train[i]*2*np.dot(weights.T, ex_train)))\n",
        "          weights = weights + lr * cross_entropy_err\n",
        "        \n",
        "          E_ins += cross_entropy_err\n",
        "        E_in_ave = E_ins / train_samp\n",
        "        E_in.append(E_in_ave)\n",
        "      \n",
        "        E_tests = 0\n",
        "        for i in test:\n",
        "          ex_test = np.append(x_test[i],1)\n",
        "          cross_entropy_err = np.log(1+np.exp(-y_test[i]*2*np.dot(weights.T, ex_test)))\n",
        "        \n",
        "        E_tests += cross_entropy_err\n",
        "        E_test_ave = E_tests / test_samp\n",
        "        E_test.append(E_test_ave)\n",
        "\n",
        "    return E_test_ave"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCvpBi0aKm2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spliting(x, y, folds):\n",
        "    #size\n",
        "    size = int(x.shape[0] / folds)\n",
        "    folds_dict = {}\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    \n",
        "    #iteration via each fold\n",
        "    for i in range(folds-1):\n",
        "        x_fold = x[size*i : size*i+size]\n",
        "        y_fold = y[size*i : size*i+size]\n",
        "        x_list.append(x_fold)\n",
        "        y_list.append(y_fold)\n",
        "   \n",
        "    #the data that left\n",
        "    x_fold = x[size*(folds-1):]\n",
        "    y_fold = y[size*(folds-1):]\n",
        "    x_list.append(x_fold)\n",
        "    y_list.append(y_fold)\n",
        "    \n",
        "    folds_dict['x'] = x_list\n",
        "    folds_dict['y'] = y_list\n",
        "    \n",
        "    return folds_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8z2yam0MdWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def estimate_lin_log(folds):\n",
        "  x_folds = np.array(folds['x'])\n",
        "  y_folds = np.array(folds['y'])\n",
        "\n",
        "  E_linear = []\n",
        "  E_logistic = []\n",
        "\n",
        "  for num in range(len(folds['x'])):\n",
        "      x_test = x_folds[num]\n",
        "      y_test = y_folds[num]\n",
        "\n",
        "      x_temp = np.delete(x_folds, num)\n",
        "      y_temp = np.delete(y_folds, num)\n",
        "\n",
        "      x_train = np.concatenate([x_temp[0]])\n",
        "      for i in range(1,len(x_temp)):\n",
        "          x_train = np.concatenate([x_train, x_temp[i]])\n",
        "\n",
        "      y_train = np.concatenate([y_temp[0]])\n",
        "      for i in range(1,len(y_temp)):\n",
        "          y_train = np.concatenate([y_train, y_temp[i]])\n",
        "        \n",
        "      E_lin = lin_reg(x_train, x_test, y_train, y_test)\n",
        "      E_linear.append(E_lin)\n",
        "    \n",
        "      E_log = log_reg(x_train, x_test, y_train, y_test)\n",
        "      E_logistic.append(E_log)\n",
        "      \n",
        "  \n",
        "      return E_linear, E_logistic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNf6NfFpOJyI",
        "colab_type": "code",
        "outputId": "2a744a07-df21-4816-a5bc-6ec5cb3857b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#10 folds cross validation\n",
        "folds = spliting(x, y, 10)\n",
        "estimate = estimate_lin_log(folds)\n",
        "estimate"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([962.5138101227963], [0.003872330617653326])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSlCdcbmPRwy",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the values of linear and logistic, logistic regression acts much better, and its error value near to 0 while Linear Regression shows much higher value almost 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weYDnD57P_Rz",
        "colab_type": "code",
        "outputId": "5a0772cd-c698-4002-854d-18b88fbdf2c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#5 fold cross validation\n",
        "folds = spliting(x, y, 5)\n",
        "estimate = estimate_lin_log(folds)\n",
        "estimate"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1574.5211666107234], [4.194725656268603e-15])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96tqz4sXRP5b",
        "colab_type": "code",
        "outputId": "6e34b1c8-24de-4f23-a722-cdc667ff3f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#20 fold cross validation\n",
        "folds = spliting(x, y, 20)\n",
        "estimate = estimate_lin_log(folds)\n",
        "estimate"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([393.75064930675194], [0.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzwlk9AlSeK1",
        "colab_type": "code",
        "outputId": "035b28ef-b754-497f-ef14-2bb3c9171ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#1 fold cross validation\n",
        "folds = spliting(x, y, 2)\n",
        "estimate = estimate_lin_log(folds)\n",
        "estimate"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3619.467132699672], [0.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfiWMHS4RxNq",
        "colab_type": "text"
      },
      "source": [
        "As we can see, changing the number of folds are changes the errors of linear regression, however logistic regression stays same with 0."
      ]
    }
  ]
}